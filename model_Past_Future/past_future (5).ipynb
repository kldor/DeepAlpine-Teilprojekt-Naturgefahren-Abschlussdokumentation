{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XATRMCTF8wMC"
      },
      "outputs": [],
      "source": [
        "# block1\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import geopandas as gpd\n",
        "import rasterio\n",
        "from rasterio.mask import mask\n",
        "from rasterio.enums import Resampling\n",
        "from shapely.geometry import box\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import shutil\n",
        "import glob\n",
        "import math\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/Landslides'\n",
        "data_dir = os.path.join(base_dir, 'Data')\n",
        "map_dir = os.path.join(base_dir, 'Output_Maps')\n",
        "temp_dir = os.path.join(base_dir, 'Temp_Processing')\n",
        "\n",
        "for d in [data_dir, map_dir, temp_dir]:\n",
        "    if not os.path.exists(d): os.makedirs(d)\n",
        "\n",
        "train_file = os.path.join(data_dir, 'landslides.csv')\n",
        "\n",
        "shapefile_path = '/content/drive/MyDrive/Landslides/Data/FME_11060556_1767623643023_2240/Muncipalities_polygon.shp'\n",
        "\n",
        "# CRU TS Links\n",
        "cru_urls = {\n",
        "    \"tmin_2010\": \"https://geodata.ucdavis.edu/climate/worldclim/2_1/hist/cts4.09/wc2.1_cruts4.09_2.5m_tmin_2010-2019.zip\",\n",
        "    \"tmin_2020\": \"https://geodata.ucdavis.edu/climate/worldclim/2_1/hist/cts4.09/wc2.1_cruts4.09_2.5m_tmin_2020-2024.zip\",\n",
        "    \"tmax_2010\": \"https://geodata.ucdavis.edu/climate/worldclim/2_1/hist/cts4.09/wc2.1_cruts4.09_2.5m_tmax_2010-2019.zip\",\n",
        "    \"tmax_2020\": \"https://geodata.ucdavis.edu/climate/worldclim/2_1/hist/cts4.09/wc2.1_cruts4.09_2.5m_tmax_2020-2024.zip\",\n",
        "    \"prec_2010\": \"https://geodata.ucdavis.edu/climate/worldclim/2_1/hist/cts4.09/wc2.1_cruts4.09_2.5m_prec_2010-2019.zip\",\n",
        "    \"prec_2020\": \"https://geodata.ucdavis.edu/climate/worldclim/2_1/hist/cts4.09/wc2.1_cruts4.09_2.5m_prec_2020-2024.zip\"\n",
        "}\n",
        "\n",
        "features = [\n",
        "    'Elevation', 'Slope', 'Aspect',\n",
        "    'BIO01_Historical_Mean', 'BIO05_Historical_Max', 'BIO06_Historical_Min',\n",
        "    'BIO12_Historical_Prec', 'BIO13_Historical_Prec', 'BIO15_Historical_Prec'\n",
        "]\n",
        "\n",
        "# Grid\n",
        "GRID_RES = 0.00416\n",
        "BBOX = (10.0, 46.0, 13.0, 47.5)\n",
        "geo_bbox = gpd.GeoDataFrame({'geometry': box(*BBOX)}, index=[0], crs=\"EPSG:4326\")\n",
        "\n",
        "# MODELL TRAINING\n",
        "print(\"\\n 1. MODELL TRAINING\")\n",
        "if not os.path.exists(train_file):\n",
        "    print(f\"FEHLER: Lade '{os.path.basename(train_file)}' nach '{data_dir}'!\")\n",
        "    exit()\n",
        "\n",
        "df_train = pd.read_csv(train_file)\n",
        "# Datum suchen für Validierung\n",
        "date_col = None\n",
        "for col in df_train.columns:\n",
        "    if any(x in col.lower() for x in ['date', 'jahr', 'year']):\n",
        "        date_col = col; break\n",
        "print(f\"Datumsspalte: {date_col}\")\n",
        "\n",
        "# Features vorbereiten\n",
        "for f in features:\n",
        "    if f not in df_train.columns: df_train[f] = 0\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=500, max_depth=15, min_samples_leaf=2,\n",
        "                            class_weight='balanced', max_features='sqrt', n_jobs=-1, random_state=42)\n",
        "rf.fit(df_train[features].fillna(0), df_train['Target'])\n",
        "print(\"Modell trainiert.\")\n",
        "\n",
        "# GITTER & TOPO\n",
        "print(\"\\n2. GITTER & TOPOGRAPHIE\")\n",
        "grid_csv = os.path.join(temp_dir, 'Grid_Topo_Only.csv')\n",
        "\n",
        "if not os.path.exists(grid_csv):\n",
        "    # Gitter erstellen\n",
        "    lon = np.arange(BBOX[0], BBOX[2], GRID_RES)\n",
        "    lat = np.arange(BBOX[1], BBOX[3], GRID_RES)\n",
        "    xx, yy = np.meshgrid(lon, lat)\n",
        "    df_grid = pd.DataFrame({'Longitude': xx.flatten(), 'Latitude': yy.flatten()})\n",
        "    print(f\"Gitter: {len(df_grid)} Punkte\")\n",
        "\n",
        "    # Elevation Download\n",
        "    elev_zip = os.path.join(temp_dir, \"elev.zip\")\n",
        "    elev_tif = os.path.join(temp_dir, \"wc2.1_30s_elev.tif\")\n",
        "\n",
        "    if not os.path.exists(elev_tif):\n",
        "        print(\"Lade DEM...\")\n",
        "        with requests.get(\"https://geodata.ucdavis.edu/climate/worldclim/2_1/base/wc2.1_30s_elev.zip\", stream=True) as r:\n",
        "            with open(elev_zip, 'wb') as f: shutil.copyfileobj(r.raw, f)\n",
        "        with zipfile.ZipFile(elev_zip, 'r') as z:\n",
        "            z.extract([n for n in z.namelist() if n.endswith('.tif')][0], temp_dir)\n",
        "            os.rename(os.path.join(temp_dir, [n for n in z.namelist() if n.endswith('.tif')][0]), elev_tif)\n",
        "\n",
        "    # Slope/Aspect berechnen\n",
        "    print(\"Berechne Slope/Aspect...\")\n",
        "    with rasterio.open(elev_tif) as src:\n",
        "        coords = [(r.Longitude, r.Latitude) for _, r in df_grid.iterrows()]\n",
        "        df_grid['Elevation'] = [x[0] for x in src.sample(coords)]\n",
        "\n",
        "        # Mask für Gradienten\n",
        "        out_img, out_transform = mask(src, geo_bbox.geometry, crop=True)\n",
        "        data = out_img[0].astype('float32')\n",
        "        res_x, res_y = out_transform[0], -out_transform[4]\n",
        "        scale_x = 111320 * math.cos(46.5*math.pi/180) * res_x\n",
        "        scale_y = 111320 * res_y\n",
        "\n",
        "        dy, dx = np.gradient(data)\n",
        "        slope = np.degrees(np.arctan(np.sqrt((dx/scale_x)**2 + (dy/scale_y)**2)))\n",
        "        aspect = (np.degrees(np.arctan2(dy, -dx)) + 360) % 360\n",
        "\n",
        "        # In Temp-Tifs schreiben für Sampling\n",
        "        meta = src.meta.copy()\n",
        "        meta.update({\"height\": data.shape[0], \"width\": data.shape[1], \"transform\": out_transform})\n",
        "        with rasterio.open(os.path.join(temp_dir, 'slope.tif'), 'w', **meta) as dst: dst.write(slope, 1)\n",
        "        with rasterio.open(os.path.join(temp_dir, 'aspect.tif'), 'w', **meta) as dst: dst.write(aspect, 1)\n",
        "\n",
        "    with rasterio.open(os.path.join(temp_dir, 'slope.tif')) as s:\n",
        "        df_grid['Slope'] = [x[0] for x in s.sample(coords)]\n",
        "    with rasterio.open(os.path.join(temp_dir, 'aspect.tif')) as a:\n",
        "        df_grid['Aspect'] = [x[0] for x in a.sample(coords)]\n",
        "\n",
        "    df_grid.to_csv(grid_csv, index=False)\n",
        "    print(\"Topo fertig.\")\n",
        "else:\n",
        "    print(\"Lade existierendes Topo-Grid...\")\n",
        "    df_grid = pd.read_csv(grid_csv)\n",
        "\n",
        "\n",
        "# DATEN (2015-2024)\n",
        "print(\"\\n3. DATEN VERARBEITUNG (2015-2024)\")\n",
        "def process_cru_downloads():\n",
        "    all_data = {'tmin': {}, 'tmax': {}, 'prec': {}}\n",
        "\n",
        "    for key, url in cru_urls.items():\n",
        "        var_type = key.split('_')[0]\n",
        "        zip_path = os.path.join(temp_dir, f\"{key}.zip\")\n",
        "        extract_path = os.path.join(temp_dir, key)\n",
        "\n",
        "        # Download\n",
        "        if not os.path.exists(extract_path):\n",
        "            print(f\"Lade {key}...\")\n",
        "            if not os.path.exists(zip_path):\n",
        "                with requests.get(url, stream=True) as r:\n",
        "                    with open(zip_path, 'wb') as f: shutil.copyfileobj(r.raw, f)\n",
        "            with zipfile.ZipFile(zip_path, 'r') as z: z.extractall(extract_path)\n",
        "            os.remove(zip_path)\n",
        "\n",
        "        # Lesen & Croppen\n",
        "        tifs = glob.glob(f\"{extract_path}/*.tif\")\n",
        "        print(f\"  Verarbeite {len(tifs)} Dateien für {key}...\")\n",
        "\n",
        "        for tif in tifs:\n",
        "            try:\n",
        "                # Dateiname parsen (manchmal '..._2010-01.tif')\n",
        "                date_part = os.path.basename(tif).replace('.tif','').split('_')[-1]\n",
        "                y, m = map(int, date_part.split('-'))\n",
        "\n",
        "                if 2015 <= y <= 2024:\n",
        "                    with rasterio.open(tif) as src:\n",
        "                        out_img, out_trans = mask(src, geo_bbox.geometry, crop=True)\n",
        "                        all_data[var_type][f\"{m:02d}_{y}\"] = (out_img[0], out_trans, src.crs, src.meta)\n",
        "            except: continue\n",
        "\n",
        "        shutil.rmtree(extract_path) # Aufräumen\n",
        "    return all_data\n",
        "\n",
        "# Daten laden\n",
        "cru_data = process_cru_downloads()\n",
        "print(\"Rohdaten im Speicher.\")\n",
        "\n",
        "# Klima berechnen\n",
        "print(\"Berechne Durchschnitte & BIOs...\")\n",
        "bio_values = {}\n",
        "# Referenz-Meta für das Upscaling später\n",
        "ref_meta = None\n",
        "\n",
        "# Mittelwerte berechnen\n",
        "monthly_means = {'tmin': [], 'tmax': [], 'prec': []}\n",
        "\n",
        "for month in range(1, 13):\n",
        "    m_str = f\"{month:02d}\"\n",
        "    for var in ['tmin', 'tmax', 'prec']:\n",
        "        arrays = []\n",
        "        for y in range(2015, 2025):\n",
        "            k = f\"{m_str}_{y}\"\n",
        "            if k in cru_data[var]:\n",
        "                data, trans, crs, meta = cru_data[var][k]\n",
        "                arrays.append(data)\n",
        "                if ref_meta is None: ref_meta = meta.copy(); ref_meta.update({\"height\": data.shape[0], \"width\": data.shape[1], \"transform\": trans})\n",
        "\n",
        "        if arrays:\n",
        "            monthly_means[var].append(np.mean(np.stack(arrays), axis=0))\n",
        "        else:\n",
        "            print(f\"WARNUNG: Keine Daten für {var} Monat {m}\")\n",
        "\n",
        "# Stapeln (12 Monate)\n",
        "tmin = np.stack(monthly_means['tmin'])\n",
        "tmax = np.stack(monthly_means['tmax'])\n",
        "prec = np.stack(monthly_means['prec'])\n",
        "\n",
        "# BIOs\n",
        "bio_values['BIO01_Historical_Mean'] = np.mean((tmax + tmin) / 2, axis=0)\n",
        "bio_values['BIO05_Historical_Max'] = np.max(tmax, axis=0)\n",
        "bio_values['BIO06_Historical_Min'] = np.min(tmin, axis=0)\n",
        "bio_values['BIO12_Historical_Prec'] = np.sum(prec, axis=0)\n",
        "bio_values['BIO13_Historical_Prec'] = np.max(prec, axis=0)\n",
        "prec_mean = np.mean(prec, axis=0)\n",
        "with np.errstate(divide='ignore', invalid='ignore'):\n",
        "    bio15 = (np.std(prec, axis=0) / prec_mean) * 100\n",
        "    bio15[prec_mean == 0] = 0\n",
        "bio_values['BIO15_Historical_Prec'] = bio15\n",
        "\n",
        "print(\"BIOs berechnet. Starte Upscaling & Mapping auf Grid...\")\n",
        "# Upscaling und Zuweisung an Grid\n",
        "coords = [(r.Longitude, r.Latitude) for _, r in df_grid.iterrows()]\n",
        "\n",
        "for name, grid_data in bio_values.items():\n",
        "    # MemoryFile für das Upscaling\n",
        "    with rasterio.io.MemoryFile() as memfile:\n",
        "        with memfile.open(**ref_meta) as dataset:\n",
        "            dataset.write(grid_data.astype('float32'), 1)\n",
        "\n",
        "            # Upscaling (Faktor 10 für ~500m)\n",
        "            upscale = 10\n",
        "            new_h = int(dataset.height * upscale)\n",
        "            new_w = int(dataset.width * upscale)\n",
        "\n",
        "            data_up = dataset.read(\n",
        "                1, out_shape=(1, new_h, new_w), resampling=Resampling.bilinear\n",
        "            )\n",
        "\n",
        "            # Neue Transformation berechnen\n",
        "            trans_up = dataset.transform * dataset.transform.scale(\n",
        "                (dataset.width / data_up.shape[-1]),\n",
        "                (dataset.height / data_up.shape[-2])\n",
        "            )\n",
        "\n",
        "            # Temporär nue\n",
        "            up_meta = dataset.meta.copy()\n",
        "            up_meta.update({\"height\": new_h, \"width\": new_w, \"transform\": trans_up})\n",
        "\n",
        "            with rasterio.io.MemoryFile() as up_mem:\n",
        "                with up_mem.open(**up_meta) as up_ds:\n",
        "                    up_ds.write(data_up, 1)\n",
        "                    # Samplen für die Gitterpunkte\n",
        "                    df_grid[name] = [x[0] for x in up_ds.sample(coords)]\n",
        "    print(f\"  -> {name} fertig.\")\n",
        "\n",
        "# Speichern\n",
        "df_grid.to_csv(os.path.join(map_dir, 'Grid_Full_Data_2015_2024.csv'), index=False)\n",
        "\n",
        "\n",
        "# VORHERSAGE & VALIDIERUNG\n",
        "print(\"\\n 4. MAPPING & VALIDIERUNG\")\n",
        "\n",
        "# Vorhersage\n",
        "print(\"Berechne Risiko...\")\n",
        "probs = rf.predict_proba(df_grid[features].fillna(0))[:, 1]\n",
        "df_grid['Landslide_Probability'] = probs\n",
        "\n",
        "# Speichern\n",
        "df_grid.to_csv(os.path.join(map_dir, 'Risk_Map_Data_2015_2024.csv'), index=False)\n",
        "\n",
        "# Basis-Plot\n",
        "plt.figure(figsize=(12, 10))\n",
        "plt.scatter(df_grid.Longitude, df_grid.Latitude, c=df_grid.Landslide_Probability, cmap='Reds', s=2, marker='s', vmin=0, vmax=1)\n",
        "plt.colorbar(label='Risk Probability')\n",
        "plt.title('Historical Landslide Susceptibility (2015-2024)')\n",
        "plt.savefig(os.path.join(map_dir, 'Map_2015_2024.png'), dpi=300)\n",
        "plt.close()\n",
        "\n",
        "# Validierung (Jahres-Overlay)\n",
        "if date_col:\n",
        "    df_train['Year'] = pd.to_datetime(df_train[date_col], errors='coerce').dt.year\n",
        "    years = sorted(df_train[df_train.Target == 1]['Year'].dropna().unique().astype(int))\n",
        "\n",
        "    print(f\"Erstelle Validierungskarten für: {years}\")\n",
        "    for year in years:\n",
        "        if year < 2015: continue\n",
        "\n",
        "        events = df_train[(df_train.Target == 1) & (df_train.Year == year)]\n",
        "        if len(events) == 0: continue\n",
        "\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        plt.scatter(df_grid.Longitude, df_grid.Latitude, c=df_grid.Landslide_Probability, cmap='Reds', s=2, marker='s', vmin=0, vmax=1, alpha=0.6)\n",
        "        plt.colorbar(label='Risk Probability')\n",
        "\n",
        "        # Events\n",
        "        plt.scatter(events.Longitude, events.Latitude, c='black', marker='x', s=100, linewidth=2, label=f'Events {year}')\n",
        "\n",
        "        plt.title(f\"Validation Year {year} (Events vs 2015-2024 Risk)\")\n",
        "        plt.legend()\n",
        "        plt.savefig(os.path.join(map_dir, f'Validation_Map_{year}.png'), dpi=300)\n",
        "        plt.close()\n",
        "        print(f\"  -> Jahr {year} fertig.\")\n",
        "\n",
        "# GEMEINDE ANALYSE (Auf Basis der 2015-2024 Karte)\n",
        "print(\"\\n 5. GEMEINDE ANALYSE\")\n",
        "if os.path.exists(shapefile_path):\n",
        "    try:\n",
        "        print(\"Lade Shapefile...\")\n",
        "        gdf_gem = gpd.read_file(shapefile_path)\n",
        "\n",
        "        # CRS Check\n",
        "        if gdf_gem.crs != \"EPSG:4326\":\n",
        "            print(f\"Transformiere Shapefile von {gdf_gem.crs} nach EPSG:4326...\")\n",
        "            gdf_gem = gdf_gem.to_crs(\"EPSG:4326\")\n",
        "\n",
        "        # Grid in GeoDataFrame umwandeln\n",
        "        print(\"Wandle Grid in Geometrie um...\")\n",
        "        gdf_points = gpd.GeoDataFrame(\n",
        "            df_grid,\n",
        "            geometry=gpd.points_from_xy(df_grid.Longitude, df_grid.Latitude),\n",
        "            crs=\"EPSG:4326\"\n",
        "        )\n",
        "\n",
        "        print(\"Spatial Join (Welcher Punkt liegt in welcher Gemeinde?)...\")\n",
        "        joined = gpd.sjoin(gdf_points, gdf_gem, how=\"inner\", predicate=\"within\")\n",
        "\n",
        "        # Namens-Spalte\n",
        "        name_col = 'NAME_DE'\n",
        "\n",
        "        print(f\"Gruppiere nach: {name_col}\")\n",
        "\n",
        "        # Aggregieren: Durchschnittliches Risiko pro Gemeinde\n",
        "        risk_gem = joined.groupby(name_col)['Landslide_Probability'].mean().reset_index()\n",
        "        risk_gem.columns = [name_col, 'Mean_Hazard']\n",
        "\n",
        "        # Zurück ins Shapefile mergen für die Karte\n",
        "        gdf_final = gdf_gem.merge(risk_gem, on=name_col)\n",
        "\n",
        "        # Plotten\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        ax = plt.gca()\n",
        "        gdf_final.plot(column='Mean_Hazard', ax=ax, cmap='OrRd', legend=True,\n",
        "                       legend_kwds={'label': \"Durchschnittl. Gefährdung (2015-2024 Climate)\"})\n",
        "        plt.title(\"Gemeinde-Gefährdungskarte\")\n",
        "        plt.savefig(os.path.join(map_dir, \"Municipality_Risk_Map.png\"), dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "        # CSV Export\n",
        "        out_csv = os.path.join(map_dir, \"Municipality_Risk_Table.csv\")\n",
        "        gdf_final[[name_col, 'Mean_Hazard']].to_csv(out_csv, index=False)\n",
        "        print(f\"Gemeinde-Daten gespeichert: {out_csv}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Fehler bei Gemeinde-Analyse: {e}\")\n",
        "else:\n",
        "    print(f\"Shapefile nicht gefunden unter: {shapefile_path}\")\n",
        "\n",
        "print(f\"\\nFERTIG! Alles gespeichert in: {map_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#block 3\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import geopandas as gpd\n",
        "import rasterio\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/Landslides'\n",
        "data_dir = os.path.join(base_dir, 'Data')\n",
        "map_dir = os.path.join(base_dir, 'Output_Maps')\n",
        "temp_dir = os.path.join(base_dir, 'Temp_Processing')\n",
        "\n",
        "shapefile_path = '/content/drive/MyDrive/Landslides/Data/FME_11060556_1767623643023_2240/Municipalities_polygon.shp'\n",
        "\n",
        "features = [\n",
        "    'Elevation', 'Slope', 'Aspect',\n",
        "    'BIO01_Historical_Mean', 'BIO05_Historical_Max', 'BIO06_Historical_Min',\n",
        "    'BIO12_Historical_Prec', 'BIO13_Historical_Prec', 'BIO15_Historical_Prec'\n",
        "]\n",
        "\n",
        "ssp_urls = {\n",
        "    'SSP126': \"https://geodata.ucdavis.edu/cmip6/30s/MPI-ESM1-2-HR/ssp126/wc2.1_30s_bioc_MPI-ESM1-2-HR_ssp126_2081-2100.tif\",\n",
        "    'SSP370': \"https://geodata.ucdavis.edu/cmip6/30s/MPI-ESM1-2-HR/ssp370/wc2.1_30s_bioc_MPI-ESM1-2-HR_ssp370_2081-2100.tif\",\n",
        "    'SSP585': \"https://geodata.ucdavis.edu/cmip6/30s/MPI-ESM1-2-HR/ssp585/wc2.1_30s_bioc_MPI-ESM1-2-HR_ssp585_2081-2100.tif\"\n",
        "}\n",
        "\n",
        "bio_map = {1:'BIO01_Historical_Mean', 5:'BIO05_Historical_Max', 6:'BIO06_Historical_Min',\n",
        "           12:'BIO12_Historical_Prec', 13:'BIO13_Historical_Prec', 15:'BIO15_Historical_Prec'}\n",
        "\n",
        "# 1. MODELL, GRID & SHAPEFILE LADEN\n",
        "# Shapefile laden (für den Hintergrund)\n",
        "if os.path.exists(shapefile_path):\n",
        "    gdf_gem = gpd.read_file(shapefile_path)\n",
        "    if gdf_gem.crs != \"EPSG:4326\": gdf_gem = gdf_gem.to_crs(\"EPSG:4326\")\n",
        "else:\n",
        "    print(\"ACHTUNG: Shapefile nicht gefunden! Karten werden ohne Grenzen erstellt.\")\n",
        "    gdf_gem = None\n",
        "\n",
        "# Grid laden\n",
        "grid_topo_path = os.path.join(temp_dir, 'Grid_Topo_Only.csv')\n",
        "if not os.path.exists(grid_topo_path):\n",
        "    print(\"FEHLER: 'Grid_Topo_Only.csv' fehlt. Bitte Teil 1 (CRU Skript) laufen lassen.\")\n",
        "    exit()\n",
        "\n",
        "df_grid_base = pd.read_csv(grid_topo_path)\n",
        "\n",
        "# Modell trainieren\n",
        "train_file = os.path.join(data_dir, 'landslides.csv')\n",
        "df_train = pd.read_csv(train_file)\n",
        "for f in features:\n",
        "    if f not in df_train.columns: df_train[f] = 0\n",
        "\n",
        "print(\"Trainiere Modell...\")\n",
        "rf = RandomForestClassifier(n_estimators=500, max_depth=15, min_samples_leaf=2,\n",
        "                            class_weight='balanced', max_features='sqrt', n_jobs=-1, random_state=42)\n",
        "rf.fit(df_train[features].fillna(0), df_train['Target'])\n",
        "\n",
        "\n",
        "# 2. ZUKUNFTS-SZENARIEN (Prediction & Mapping)\n",
        "results = {}\n",
        "\n",
        "for name, url in ssp_urls.items():\n",
        "    print(f\"\\n--- Verarbeite {name} ---\")\n",
        "    df_scen = df_grid_base.copy()\n",
        "    coords = [(r.Longitude, r.Latitude) for _, r in df_scen.iterrows()]\n",
        "\n",
        "    print(\"Streame Klimadaten...\")\n",
        "    with rasterio.open(url) as src:\n",
        "        for b_idx, col in bio_map.items():\n",
        "            if b_idx <= src.count:\n",
        "                df_scen[col] = [x[0] for x in src.sample(coords, indexes=b_idx)]\n",
        "            else:\n",
        "                df_scen[col] = 0\n",
        "\n",
        "    print(\"Vorhersage...\")\n",
        "    probs = rf.predict_proba(df_scen[features].fillna(0))[:, 1]\n",
        "    df_scen['Landslide_Probability'] = probs\n",
        "\n",
        "    # Speichern CSV\n",
        "    df_scen.to_csv(os.path.join(map_dir, f'Result_Grid_{name}.csv'), index=False)\n",
        "\n",
        "    # PLOTTING MIT HINTERGRUND\n",
        "    print(f\"Erstelle Karte für {name}...\")\n",
        "    fig, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "    # 1. Die Risikokarte (Scatter)\n",
        "    sc = ax.scatter(df_scen.Longitude, df_scen.Latitude,\n",
        "                    c=df_scen.Landslide_Probability,\n",
        "                    cmap='Reds', vmin=0, vmax=1,\n",
        "                    s=12, marker='s', zorder=1)\n",
        "\n",
        "    # 2. Gemeindegrenzen darüberlegen (Overlay)\n",
        "    if gdf_gem is not None:\n",
        "        gdf_gem.plot(ax=ax, facecolor='none', edgecolor='black', linewidth=0.3, alpha=0.6, zorder=2)\n",
        "\n",
        "    plt.colorbar(sc, label='Probability')\n",
        "    plt.title(f'Future Projection: {name} (2081-2100)')\n",
        "    plt.xlabel('Longitude')\n",
        "    plt.ylabel('Latitude')\n",
        "\n",
        "    out_png = os.path.join(map_dir, f'Map_{name}.png')\n",
        "    plt.savefig(out_png, dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    # Für SSP3 das DF für die Gemeinde-Analyse speichern\n",
        "    if name == 'SSP370':\n",
        "        results['SSP3'] = df_scen\n",
        "\n",
        "\n",
        "# 3. GEMEINDE ANALYSE\n",
        "print(\"\\n GEMEINDE ANALYSE (SSP3)\")\n",
        "if 'SSP3' in results and gdf_gem is not None:\n",
        "    try:\n",
        "        df_ssp3 = results['SSP3']\n",
        "        gdf_points = gpd.GeoDataFrame(df_ssp3, geometry=gpd.points_from_xy(df_ssp3.Longitude, df_ssp3.Latitude), crs=\"EPSG:4326\")\n",
        "\n",
        "        print(\"Spatial Join...\")\n",
        "        joined = gpd.sjoin(gdf_points, gdf_gem, how=\"inner\", predicate=\"within\")\n",
        "\n",
        "        # Name finden\n",
        "        name_col = 'NAME_DE'\n",
        "        if name_col not in joined.columns:\n",
        "            cands = [c for c in joined.columns if 'NAME' in c]\n",
        "            name_col = cands[0] if cands else joined.columns[0]\n",
        "\n",
        "        print(f\"Aggregiere nach {name_col}...\")\n",
        "        risk_gem = joined.groupby(name_col)['Landslide_Probability'].mean().reset_index()\n",
        "        risk_gem.columns = [name_col, 'Mean_Hazard_SSP3']\n",
        "\n",
        "        gdf_final = gdf_gem.merge(risk_gem, on=name_col)\n",
        "\n",
        "        # Plot (die Gemeinden eingefärbt)\n",
        "        fig, ax = plt.subplots(figsize=(12, 10))\n",
        "        gdf_final.plot(column='Mean_Hazard_SSP3', ax=ax, cmap='OrRd',\n",
        "                       legend=True, legend_kwds={'label': \"Durchschnittl. Gefährdung (SSP3)\"},\n",
        "                       edgecolor='black', linewidth=0.2)\n",
        "\n",
        "        plt.title(\"Gefährdung pro Gemeinde (Szenario SSP3-7.0)\")\n",
        "        plt.savefig(os.path.join(map_dir, \"Municipality_Risk_SSP3.png\"), dpi=300)\n",
        "\n",
        "        gdf_final[[name_col, 'Mean_Hazard_SSP3']].to_csv(os.path.join(map_dir, \"Municipality_SSP3_Table.csv\"), index=False)\n",
        "        print(\"Gemeinde SSP3 Karte fertig.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Fehler Gemeinde Analyse: {e}\")\n",
        "\n",
        "print(\"\\nFERTIG!\")"
      ],
      "metadata": {
        "id": "X_f6eykQcW7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# block13\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "import os\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/Landslides'\n",
        "data_dir = os.path.join(base_dir, 'Data')\n",
        "train_file = os.path.join(data_dir, 'landslides.csv')\n",
        "\n",
        "features = [\n",
        "    'Elevation', 'Slope', 'Aspect',\n",
        "    'BIO01_Historical_Mean', 'BIO05_Historical_Max', 'BIO06_Historical_Min',\n",
        "    'BIO12_Historical_Prec', 'BIO13_Historical_Prec', 'BIO15_Historical_Prec'\n",
        "]\n",
        "\n",
        "df = pd.read_csv(train_file)\n",
        "X = df[features]\n",
        "y = df['Target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(\"Searching for best model parameters (5-Fold CV)...\")\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [3, 4, 5, 6, 7],\n",
        "    'min_samples_leaf': [5, 8, 10, 15],\n",
        "    'n_estimators': [200, 300],\n",
        "    'max_features': ['sqrt']\n",
        "}\n",
        "\n",
        "rf = RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1)\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='f1',\n",
        "    return_train_score=True,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "results = pd.DataFrame(grid.cv_results_)\n",
        "results['overfitting_gap'] = results['mean_train_score'] - results['mean_test_score']\n",
        "\n",
        "valid_models = results[results['overfitting_gap'] < 0.16]\n",
        "\n",
        "if not valid_models.empty:\n",
        "    best_row = valid_models.sort_values(by='mean_test_score', ascending=False).iloc[0]\n",
        "    best_params = best_row['params']\n",
        "    print(\"\\nSUCCESS: Found a model satisfying gap < 16%.\")\n",
        "else:\n",
        "    print(\"\\nWARNING: No model satisfied gap < 16%. Selecting model with lowest gap.\")\n",
        "    best_row = results.sort_values(by='overfitting_gap', ascending=True).iloc[0]\n",
        "    best_params = best_row['params']\n",
        "\n",
        "print(\"-\" * 60)\n",
        "print(\"BEST HYPERPARAMETERS FOUND\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"Max Depth:        {best_params['max_depth']}\")\n",
        "print(f\"Min Samples Leaf: {best_params['min_samples_leaf']}\")\n",
        "print(f\"N Estimators:     {best_params['n_estimators']}\")\n",
        "print(f\"CV Train F1:      {best_row['mean_train_score']:.4f}\")\n",
        "print(f\"CV Val F1:        {best_row['mean_test_score']:.4f}\")\n",
        "print(f\"Overfitting Gap:  {best_row['overfitting_gap']:.4f}\")\n",
        "\n",
        "final_model = RandomForestClassifier(\n",
        "    **best_params,\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "final_model.fit(X_train, y_train)\n",
        "y_test_pred = final_model.predict(X_test)\n",
        "test_f1 = f1_score(y_test, y_test_pred)\n",
        "\n",
        "print(\"-\" * 60)\n",
        "print(\"FINAL TEST SET EVALUATION\")\n",
        "print(f\"Test F1-Score: {test_f1:.4f}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_test_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_test_pred))"
      ],
      "metadata": {
        "id": "HGDfOitLu7qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# block14\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "base_dir = '/content/drive/MyDrive/Landslides'\n",
        "data_dir = os.path.join(base_dir, 'Data')\n",
        "train_file = os.path.join(data_dir, 'landslides.csv')\n",
        "\n",
        "# Feature Sets\n",
        "features_full = [\n",
        "    'Elevation', 'Slope', 'Aspect',\n",
        "    'BIO01_Historical_Mean', 'BIO05_Historical_Max', 'BIO06_Historical_Min',\n",
        "    'BIO12_Historical_Prec', 'BIO13_Historical_Prec', 'BIO15_Historical_Prec'\n",
        "]\n",
        "\n",
        "features_no_aspect = [f for f in features_full if f != 'Aspect']\n",
        "\n",
        "df = pd.read_csv(train_file)\n",
        "y = df['Target']\n",
        "\n",
        "# 1. Evaluate Model WITH Aspect\n",
        "print(\"Training Model A: WITH Aspect...\")\n",
        "X_full = df[features_full]\n",
        "X_train_A, X_test_A, y_train_A, y_test_A = train_test_split(X_full, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "rf_A = RandomForestClassifier(\n",
        "    n_estimators=300, max_depth=7, min_samples_leaf=5,\n",
        "    class_weight='balanced', max_features='sqrt',\n",
        "    n_jobs=-1, random_state=42\n",
        ")\n",
        "rf_A.fit(X_train_A, y_train_A)\n",
        "y_pred_A = rf_A.predict(X_test_A)\n",
        "\n",
        "# 2. Evaluate Model WITHOUT Aspect\n",
        "print(\"Training Model B: WITHOUT Aspect...\")\n",
        "X_no_aspect = df[features_no_aspect]\n",
        "X_train_B, X_test_B, y_train_B, y_test_B = train_test_split(X_no_aspect, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "rf_B = RandomForestClassifier(\n",
        "    n_estimators=300, max_depth=7, min_samples_leaf=5,\n",
        "    class_weight='balanced', max_features='sqrt',\n",
        "    n_jobs=-1, random_state=42\n",
        ")\n",
        "rf_B.fit(X_train_B, y_train_B)\n",
        "y_pred_B = rf_B.predict(X_test_B)\n",
        "\n",
        "# 3. Comparison Report\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"{'METRIC':<15} {'WITH ASPECT':<15} {'WITHOUT ASPECT':<15} {'DIFFERENCE':<15}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "metrics = {\n",
        "    'Accuracy': accuracy_score,\n",
        "    'Precision': precision_score,\n",
        "    'Recall': recall_score,\n",
        "    'F1-Score': f1_score\n",
        "}\n",
        "\n",
        "for name, func in metrics.items():\n",
        "    score_A = func(y_test_A, y_pred_A)\n",
        "    score_B = func(y_test_B, y_pred_B)\n",
        "    diff = score_A - score_B\n",
        "    print(f\"{name:<15} {score_A:.4f}          {score_B:.4f}          {diff:+.4f}\")\n",
        "\n",
        "print(\"-\" * 60)\n",
        "print(\"Feature Importance (Mit Aspect):\")\n",
        "importances = rf_A.feature_importances_\n",
        "for i, name in enumerate(features_full):\n",
        "    if name == 'Aspect':\n",
        "        print(f\"Aspect Importance: {importances[i]:.4f}\")"
      ],
      "metadata": {
        "id": "en7OC9sc2geN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# block15\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import seaborn as sns\n",
        "import geopandas as gpd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "base_dir = '/content/drive/MyDrive/Landslides'\n",
        "data_dir = os.path.join(base_dir, 'Data')\n",
        "map_dir = os.path.join(base_dir, 'Output_Maps')\n",
        "train_file = os.path.join(data_dir, 'landslides.csv')\n",
        "shapefile_path = os.path.join(data_dir, 'FME_11060556_1767623643023_2240/Municipalities_polygon.shp')\n",
        "\n",
        "# os.makedirs(map_dir, exist_ok=True)\n",
        "\n",
        "# MODEL TRAINING\n",
        "\n",
        "features = [\n",
        "    'Elevation', 'Slope', 'Aspect',\n",
        "    'BIO01_Historical_Mean', 'BIO05_Historical_Max', 'BIO06_Historical_Min',\n",
        "    'BIO12_Historical_Prec', 'BIO13_Historical_Prec', 'BIO15_Historical_Prec'\n",
        "]\n",
        "\n",
        "df = pd.read_csv(train_file)\n",
        "X = df[features]\n",
        "y = df['Target']\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(\"Training Optimized Model (Depth=7, MinSamples=5)...\")\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=7,\n",
        "    min_samples_leaf=5,\n",
        "    class_weight='balanced',\n",
        "    max_features='sqrt',\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# METRICS REPORT\n",
        "y_test_pred = rf.predict(X_test)\n",
        "y_train_pred = rf.predict(X_train)\n",
        "train_acc = accuracy_score(y_train, y_train_pred)\n",
        "test_acc = accuracy_score(y_test, y_test_pred)\n",
        "prec = precision_score(y_test, y_test_pred)\n",
        "rec = recall_score(y_test, y_test_pred)\n",
        "f1 = f1_score(y_test, y_test_pred)\n",
        "\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"MODEL REPORT\")\n",
        "print(\"1. OVERFITTING CHECK\")\n",
        "print(f\"Train Acc: {train_acc:.4f}\")\n",
        "print(f\"Test Acc:  {test_acc:.4f}\")\n",
        "print(f\"Gap:       {train_acc - test_acc:.4f}\")\n",
        "print(\"2. TEST METRICS\")\n",
        "print(f\"Precision: {prec:.4f}\")\n",
        "print(f\"Recall:    {rec:.4f}\")\n",
        "print(f\"F1-Score:  {f1:.4f}\")\n",
        "print(\"3. CLASSIFICATION REPORT\")\n",
        "print(classification_report(y_test, y_test_pred))\n",
        "\n",
        "# Save Confusion Matrix\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.title('Confusion Matrix (Test Set)')\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.savefig(os.path.join(map_dir, 'confusion_matrix.png'), dpi=300)\n",
        "plt.close()\n",
        "\n",
        "# Save Feature Importance\n",
        "importances = rf.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "names = [features[i] for i in indices]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(range(len(features)), importances[indices], align='center', color='#4c72b0')\n",
        "plt.yticks(range(len(features)), names)\n",
        "plt.title('Feature Importance (RF Optimized)')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(map_dir, 'feature_importance.png'), dpi=300)\n",
        "plt.close()\n",
        "\n",
        "# Text Output for Features\n",
        "print(\"4. FEATURE IMPORTANCE\")\n",
        "for i in range(len(features)):\n",
        "    print(f\"{i+1}. {names[i]:<25} {importances[indices[i]]:.4f}\")\n",
        "\n",
        "# UPDATE GRID FILES & GENERATE STATS\n",
        "files_to_update = {\n",
        "    'Historical': 'Risk_Map_Data_2015_2024.csv',\n",
        "    'SSP1-2.6': 'Result_Grid_SSP126.csv',\n",
        "    'SSP3-7.0': 'Result_Grid_SSP370.csv',\n",
        "    'SSP5-8.5': 'Result_Grid_SSP585.csv'\n",
        "}\n",
        "\n",
        "data_frames = {}\n",
        "high_risk_shares = {}\n",
        "\n",
        "for name, filename in files_to_update.items():\n",
        "    path = os.path.join(map_dir, filename)\n",
        "    if os.path.exists(path):\n",
        "        print(f\"Updating {name} ({filename})...\")\n",
        "        df_grid = pd.read_csv(path)\n",
        "\n",
        "        # Check if features exist\n",
        "        if all(f in df_grid.columns for f in features):\n",
        "            # PREDICT NEW PROBABILITIES\n",
        "            probs = rf.predict_proba(df_grid[features])[:, 1]\n",
        "            df_grid['Landslide_Probability'] = probs\n",
        "\n",
        "            # Save back to drive\n",
        "            df_grid.to_csv(path, index=False)\n",
        "            data_frames[name] = df_grid\n",
        "\n",
        "            # Store stats for plotting\n",
        "            high_risk_shares[name] = (probs > 0.5).mean()\n",
        "        else:\n",
        "            print(f\"  -> ERROR: Missing features in {filename}. Skipping.\")\n",
        "    else:\n",
        "        print(f\"  -> WARNING: File {filename} not found. Cannot update or plot.\")\n",
        "\n",
        "# PLOT: Risk Density\n",
        "if data_frames:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for name, df_scen in data_frames.items():\n",
        "        sns.kdeplot(df_scen['Landslide_Probability'], label=name, fill=True, alpha=0.1)\n",
        "\n",
        "    plt.title('Risk Distribution Density by Scenario')\n",
        "    plt.xlabel('Landslide Probability')\n",
        "    plt.xlim(0, 1)\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(map_dir, 'risk_density_distribution.png'), dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "# PLOT: High Risk Share\n",
        "if high_risk_shares:\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    names_bar = list(high_risk_shares.keys())\n",
        "    values = [high_risk_shares[n] * 100 for n in names_bar] # Convert to %\n",
        "    plt.bar(names_bar, values, color=['gray', 'green', 'orange', 'red'])\n",
        "    plt.ylabel('Share of High-Risk Area (%)')\n",
        "    plt.title('Share of Area with Probability > 0.5')\n",
        "    for i, v in enumerate(values):\n",
        "        plt.text(i, v + 0.1, f\"{v:.1f}%\", ha='center')\n",
        "    plt.savefig(os.path.join(map_dir, 'high_risk_share_comparison.png'), dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "# MAP GENERATION (MANUAL LAYOUT)\n",
        "print(\"GENERATING MAPS\")\n",
        "# MANUAL CONFIG\n",
        "LEFT_MARGIN = 0.8\n",
        "RIGHT_MARGIN = 1.2\n",
        "BAR_GAP = 0.4\n",
        "BAR_WIDTH = 0.25\n",
        "\n",
        "colors = [\"#009900\", \"#FFFF00\", \"#FF0000\"]\n",
        "custom_cmap = mcolors.LinearSegmentedColormap.from_list(\"TrafficLight\", colors)\n",
        "\n",
        "# Load Shapefile & Calc Geometry\n",
        "if os.path.exists(shapefile_path):\n",
        "    gdf_gem = gpd.read_file(shapefile_path).to_crs(\"EPSG:4326\")\n",
        "    minx, miny, maxx, maxy = gdf_gem.total_bounds\n",
        "\n",
        "    zoom_xlim = (minx - 0.02, maxx + 0.02)\n",
        "    zoom_ylim = (miny - 0.02, maxy + 0.02)\n",
        "\n",
        "    zoom_width = zoom_xlim[1] - zoom_xlim[0]\n",
        "    zoom_height = zoom_ylim[1] - zoom_ylim[0]\n",
        "    mean_lat = (zoom_ylim[0] + zoom_ylim[1]) / 2\n",
        "    geo_factor = 1 / np.cos(np.deg2rad(mean_lat))\n",
        "\n",
        "    target_aspect = (zoom_width / zoom_height) / geo_factor\n",
        "\n",
        "    map_height_in = 6\n",
        "    map_width_in = map_height_in * target_aspect\n",
        "\n",
        "    total_height = map_height_in + 1.0\n",
        "    total_width = LEFT_MARGIN + map_width_in + BAR_GAP + BAR_WIDTH + RIGHT_MARGIN\n",
        "    figsize_manual = (total_width, total_height)\n",
        "else:\n",
        "    print(\"Shapefile not found.\")\n",
        "    exit()\n",
        "\n",
        "def plot_final_layout(data, col_name, title, filename, is_shapefile=False, v_max=1.0, legend_label='', overlay_events=False):\n",
        "    fig = plt.figure(figsize=figsize_manual)\n",
        "\n",
        "    # Axes Positions\n",
        "    ax_left = LEFT_MARGIN / total_width\n",
        "    ax_bottom = 0.5 / total_height\n",
        "    ax_width = map_width_in / total_width\n",
        "    ax_height = map_height_in / total_height\n",
        "\n",
        "    cbar_left = (LEFT_MARGIN + map_width_in + BAR_GAP) / total_width\n",
        "    cbar_width = BAR_WIDTH / total_width\n",
        "\n",
        "    ax = fig.add_axes([ax_left, ax_bottom, ax_width, ax_height])\n",
        "    cax = fig.add_axes([cbar_left, ax_bottom, cbar_width, ax_height])\n",
        "\n",
        "    # Plot Content\n",
        "    if is_shapefile:\n",
        "        im = data.plot(column=col_name, ax=ax, cmap=custom_cmap, vmin=0, vmax=v_max,\n",
        "                       edgecolor='black', linewidth=0.3).collections[0]\n",
        "    else:\n",
        "        # Pixel Map\n",
        "        mask = (data.Longitude >= zoom_xlim[0]) & (data.Longitude <= zoom_xlim[1]) & \\\n",
        "               (data.Latitude >= zoom_ylim[0]) & (data.Latitude <= zoom_ylim[1])\n",
        "        df_plot = data[mask]\n",
        "\n",
        "        im = ax.scatter(df_plot.Longitude, df_plot.Latitude,\n",
        "                        c=df_plot[col_name],\n",
        "                        cmap=custom_cmap, vmin=0, vmax=v_max,\n",
        "                        s=15, marker='s', zorder=1, alpha=0.8)\n",
        "\n",
        "        # Solid Outlines (alpha=1.0)\n",
        "        gdf_gem.plot(ax=ax, facecolor='none', edgecolor='black', linewidth=0.5, alpha=1.0, zorder=2)\n",
        "\n",
        "    # Landslide Overlay (Target=1 from landslides.csv)\n",
        "    if overlay_events:\n",
        "        df_ls = pd.read_csv(train_file)\n",
        "        if 'Target' in df_ls.columns:\n",
        "            events = df_ls[df_ls['Target'] == 1]\n",
        "            # Clip to map view\n",
        "            events = events[\n",
        "                (events.Longitude >= zoom_xlim[0]) & (events.Longitude <= zoom_xlim[1]) &\n",
        "                (events.Latitude >= zoom_ylim[0]) & (events.Latitude <= zoom_ylim[1])\n",
        "            ]\n",
        "            ax.scatter(events.Longitude, events.Latitude,\n",
        "                       c='black', marker='x', s=35, linewidth=1.0,\n",
        "                       label='Recorded Landslides', zorder=3)\n",
        "            ax.legend(loc='upper right', fontsize=9, framealpha=0.9, edgecolor='black')\n",
        "\n",
        "    # Colorbar\n",
        "    cbar = plt.colorbar(im, cax=cax)\n",
        "    cbar.set_label(legend_label, rotation=270, labelpad=15)\n",
        "\n",
        "    # Styling\n",
        "    ax.set_xlim(zoom_xlim)\n",
        "    ax.set_ylim(zoom_ylim)\n",
        "    ax.set_title(title, fontsize=12)\n",
        "    ax.set_xlabel('Longitude')\n",
        "    ax.set_ylabel('Latitude')\n",
        "    ax.set_aspect(geo_factor)\n",
        "\n",
        "    save_path = os.path.join(map_dir, filename)\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    plt.close()\n",
        "    print(f\"Saved: {filename}\")\n",
        "\n",
        "# EXECUTE MAPS\n",
        "# 1. Historical\n",
        "if 'Historical' in data_frames:\n",
        "    plot_final_layout(data_frames['Historical'], 'Landslide_Probability',\n",
        "                      \"Historical Susceptibility (2015-2024)\", \"Map_Historical_new.png\",\n",
        "                      legend_label='Landslide Probability', overlay_events=True)\n",
        "\n",
        "# 2. Scenarios\n",
        "for name in ['SSP1-2.6', 'SSP3-7.0', 'SSP5-8.5']:\n",
        "    if name in data_frames:\n",
        "        code = name.replace('-', '').replace('.', '')\n",
        "        plot_final_layout(data_frames[name], 'Landslide_Probability',\n",
        "                          f\"Future Projection: {name} (2081-2100)\", f\"Map_{code}_new.png\",\n",
        "                          legend_label='Landslide Probability')\n",
        "\n",
        "# 3. Municipality (SSP3)\n",
        "if 'SSP3-7.0' in data_frames:\n",
        "    df_grid = data_frames['SSP3-7.0']\n",
        "    gdf_points = gpd.GeoDataFrame(df_grid, geometry=gpd.points_from_xy(df_grid.Longitude, df_grid.Latitude), crs=\"EPSG:4326\")\n",
        "    joined = gpd.sjoin(gdf_points, gdf_gem, how=\"inner\", predicate=\"within\")\n",
        "\n",
        "    col = 'NAME_DE' if 'NAME_DE' in joined.columns else joined.columns[0]\n",
        "    # Calculate Share > 0.5\n",
        "    risk_gem = joined.groupby(col)['Landslide_Probability'].apply(lambda x: (x > 0.5).mean()).reset_index()\n",
        "    gdf_final = gdf_gem.merge(risk_gem.rename(columns={'Landslide_Probability': 'High_Risk_Share'}), on=col)\n",
        "\n",
        "    plot_final_layout(gdf_final, 'High_Risk_Share',\n",
        "                      \"Municipality High-Risk Zones (SSP3-7.0)\",\n",
        "                      \"Municipality_Risk_Share_SSP3_new.png\",\n",
        "                      is_shapefile=True, v_max=0.5,\n",
        "                      legend_label='Share of High-Risk Area (p>0.5)')\n",
        "\n",
        "print(\"\\nProcess Complete. All outputs in:\", map_dir)"
      ],
      "metadata": {
        "id": "1SK5pO9L3Nys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# block18\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "base_dir = '/content/drive/MyDrive/Landslides'\n",
        "map_dir = os.path.join(base_dir, 'Output_Maps')\n",
        "\n",
        "scenarios = {\n",
        "    'Historical': 'Risk_Map_Data_2015_2024.csv',\n",
        "    'SSP1-2.6': 'Result_Grid_SSP126.csv',\n",
        "    'SSP3-7.0': 'Result_Grid_SSP370.csv',\n",
        "    'SSP5-8.5': 'Result_Grid_SSP585.csv'\n",
        "}\n",
        "# UPDATE GRID FILES & GENERATE STATS\n",
        "\n",
        "files_to_update = {\n",
        "    'Historical': 'Risk_Map_Data_2015_2024.csv',\n",
        "    'SSP1-2.6': 'Result_Grid_SSP126.csv',\n",
        "    'SSP3-7.0': 'Result_Grid_SSP370.csv',\n",
        "    'SSP5-8.5': 'Result_Grid_SSP585.csv'\n",
        "}\n",
        "\n",
        "data_frames = {}\n",
        "stats_list = []\n",
        "\n",
        "for name, filename in files_to_update.items():\n",
        "    path = os.path.join(map_dir, filename)\n",
        "    if os.path.exists(path):\n",
        "        print(f\"Updating {name} ({filename})...\")\n",
        "        df_grid = pd.read_csv(path)\n",
        "\n",
        "        # Check if features exist\n",
        "        if all(f in df_grid.columns for f in features):\n",
        "            probs = rf.predict_proba(df_grid[features])[:, 1]\n",
        "            df_grid['Landslide_Probability'] = probs\n",
        "\n",
        "            # Add 'Scenario' column for Violin Plot\n",
        "            df_grid['Scenario'] = name\n",
        "\n",
        "            # Save back to drive\n",
        "            df_grid.to_csv(path, index=False)\n",
        "            data_frames[name] = df_grid\n",
        "\n",
        "            # Calculate Statistics for the Table\n",
        "            mean_risk = probs.mean()\n",
        "            high_risk_share = (probs > 0.5).mean()\n",
        "\n",
        "            stats_list.append({\n",
        "                'Scenario': name,\n",
        "                'Mean_Probability': mean_risk,\n",
        "                'High_Risk_Share_Percent': high_risk_share * 100\n",
        "            })\n",
        "        else:\n",
        "            print(f\"  -> ERROR: Missing features in {filename}.\")\n",
        "    else:\n",
        "        print(f\"  -> WARNING: File {filename} not found. Cannot update or plot.\")\n",
        "\n",
        "# SAVE STATISTICS TABLE\n",
        "if stats_list:\n",
        "    stats_df = pd.DataFrame(stats_list)\n",
        "    stats_out = os.path.join(map_dir, 'Scenario_Statistics_Table.csv')\n",
        "    stats_df.to_csv(stats_out, index=False)\n",
        "    print(f\"\\n-> Statistics Table saved to: {stats_out}\")\n",
        "    print(stats_df.to_string(index=False))\n",
        "\n",
        "# PLOT: Violin Plot\n",
        "if data_frames:\n",
        "    print(\"\\nGenerating Violin Plot...\")\n",
        "    # Combine data for plotting\n",
        "    df_all = pd.concat([d[['Landslide_Probability', 'Scenario']] for d in data_frames.values()])\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    # Colors: Gray (Hist), Green (SSP1), Orange (SSP3), Red (SSP5)\n",
        "    palette = ['#A9A9A9', '#009900', '#FF8C00', '#FF0000']\n",
        "\n",
        "    sns.violinplot(data=df_all, x='Scenario', y='Landslide_Probability',\n",
        "                   palette=palette, inner='quartile', linewidth=1.5)\n",
        "\n",
        "    plt.title('Distribution of Landslide Risk (Violin Plot)', fontsize=15)\n",
        "    plt.ylabel('Landslide Probability (0-1)', fontsize=12)\n",
        "    plt.ylim(0, 1)\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    plt.savefig(os.path.join(map_dir, 'risk_distribution_violin.png'), dpi=300)\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "bMNm5H_D7EIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import os\n",
        "import geopandas as gpd\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "base_dir = '/content/drive/MyDrive/Landslides'\n",
        "map_dir = os.path.join(base_dir, 'Output_Maps')\n",
        "\n",
        "# Results in numbers\n",
        "\n",
        "# 1. Confusion Matrix\n",
        "try:\n",
        "    print(\"\\n MODELLPERFORMANZ\")\n",
        "    cm = confusion_matrix(y_test, y_test_pred)\n",
        "    print(f\"Wahr-Negativ (Sicher als Sicher erkannt): {cm[0][0]}\")\n",
        "    print(f\"Falsch-Positiv (Fehlalarm): {cm[0][1]}\")\n",
        "    print(f\"Falsch-Negativ (Erdrutsch übersehen): {cm[1][0]}\")\n",
        "    print(f\"Wahr-Positiv (Erdrutsch korrekt erkannt): {cm[1][1]}\")\n",
        "    print(\"\\nKlassifikationsbericht:\")\n",
        "    print(classification_report(y_test, y_test_pred))\n",
        "except NameError:\n",
        "    print(\"Fehler: y_test nicht gefunden.\")\n",
        "\n",
        "# 2. Feature Importance Ranking\n",
        "try:\n",
        "    print(\"\\n FEATURE IMPORTANCE RANKING\")\n",
        "    importances = rf.feature_importances_\n",
        "    indices = np.argsort(importances)[::-1]\n",
        "    for i in range(len(features)):\n",
        "        print(f\"{i+1}. {features[indices[i]]}: {importances[indices[i]]:.4f} ({importances[indices[i]]*100:.1f} generalised%) \")\n",
        "except NameError:\n",
        "    print(\"Fehler: Das Modell 'rf' wurde nicht gefunden.\")\n",
        "\n",
        "# 3. Räumliche Hotspots (Gemeinde-Auswertung Historisch vs. SSP5-8.5)\n",
        "try:\n",
        "    print(\"\\n RÄUMLICHE HOTSPOTS (GEMEINDEN)\")\n",
        "\n",
        "    # Shapefile laden\n",
        "    gdf_gem = gpd.read_file(shapefile_path).to_crs(\"EPSG:4326\")\n",
        "    col = 'NAME_DE' if 'NAME_DE' in gdf_gem.columns else gdf_gem.columns[0]\n",
        "\n",
        "    # Historisch berechnen\n",
        "    df_hist = pd.read_csv(os.path.join(map_dir, 'Risk_Map_Data_2015_2024.csv'))\n",
        "    gdf_hist = gpd.GeoDataFrame(df_hist, geometry=gpd.points_from_xy(df_hist.Longitude, df_hist.Latitude), crs=\"EPSG:4326\")\n",
        "    joined_hist = gpd.sjoin(gdf_hist, gdf_gem, how=\"inner\", predicate=\"within\")\n",
        "    risk_hist = joined_hist.groupby(col)['Landslide_Probability'].apply(lambda x: (x > 0.5).mean() * 100).reset_index()\n",
        "    risk_hist.columns = [col, 'Gefahrenzone_Historisch_Prozent']\n",
        "\n",
        "    # SSP3-7.0\n",
        "    df_ssp3 = pd.read_csv(os.path.join(map_dir, 'Result_Grid_SSP370.csv'))\n",
        "    gdf_ssp3 = gpd.GeoDataFrame(df_ssp3, geometry=gpd.points_from_xy(df_ssp3.Longitude, df_ssp3.Latitude), crs=\"EPSG:4326\")\n",
        "    joined_ssp3 = gpd.sjoin(gdf_ssp3, gdf_gem, how=\"inner\", predicate=\"within\")\n",
        "    risk_ssp3 = joined_ssp3.groupby(col)['Landslide_Probability'].apply(lambda x: (x > 0.5).mean() * 100).reset_index()\n",
        "    risk_ssp3.columns = [col, 'Gefahrenzone_SSP3_Prozent']\n",
        "\n",
        "    # Vergleichen\n",
        "    comparison = pd.merge(risk_hist, risk_ssp3, on=col)\n",
        "    comparison['Zunahme_Prozentpunkte'] = comparison['Gefahrenzone_SSP3_Prozent'] - comparison['Gefahrenzone_Historisch_Prozent']\n",
        "\n",
        "    print(\"\\nTop 3 Gemeinden in der Historischen Baseline (Höchster Anteil an Zonen > 0.5):\")\n",
        "    print(comparison.sort_values(by='Gefahrenzone_Historisch_Prozent', ascending=False)[[col, 'Gefahrenzone_Historisch_Prozent']].head(3).to_string(index=False))\n",
        "\n",
        "    print(\"\\nTop 3 Gemeinden mit dem stärkten Anstieg im Szenario SSP3-7.0:\")\n",
        "    print(comparison.sort_values(by='Zunahme_Prozentpunkte', ascending=False)[[col, 'Gefahrenzone_Historisch_Prozent', 'Gefahrenzone_SSP3_Prozent', 'Zunahme_Prozentpunkte']].head(3).to_string(index=False))\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nInfo: Gemeinde-Auswertung übersprungen (Fehler oder Dateien fehlen: {e})\")"
      ],
      "metadata": {
        "id": "xgmAXOenbPJk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}